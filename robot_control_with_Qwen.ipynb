{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad625f-d4d2-4f0b-8d23-1b279e861d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3eff805-e58a-4132-922e-aa59f7b39da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qwen-vl-utils==0.0.10\n",
    "# !pip install httpx==0.27.2\n",
    "# !pip install openai==1.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103b56bf-f8fe-4006-a50d-cf30e8368894",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input your instruction:  pick up the Paper tube on top of the cell phone\n"
     ]
    }
   ],
   "source": [
    "# Your Command\n",
    "command = input(\"Please input your instruction: \")\n",
    "# pick up the Paper tube on top of the cell phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73f155f-6dd3-460a-85ad-c522ff85de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Qwen API_KEY\n",
    "import yaml\n",
    "apikey = yaml.safe_load(open('env/configs.yaml', 'r'))[\"qwen_apikey\"]\n",
    "\n",
    "\n",
    "# the whole prompt in english\n",
    "prompt = \"\"\"I am about to give a command to the robot arm. Please help me extract the starting object and the ending object from this sentence, find the pixel coordinates of the upper left corner and the lower right corner of the two objects from this picture, and output the json data structure.\n",
    "\n",
    "For example, if my command is: Please help me put the red square on the house sketch.\n",
    "You output the following format:\n",
    "{\n",
    "\"start\":\"red square\",\n",
    "\"start_xyxy\":[[102,505],[324,860]],\n",
    "\"end\":\"house sketch\",\n",
    "\"end_xyxy\":[[300,150],[476,310]]\n",
    "}\n",
    "\n",
    "Just reply to the json itself, don't reply to other content\n",
    "\n",
    "My current command is: \"\"\" + command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fed478-65c7-4796-b497-e9e7b86b364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# API_KEY for Qwen model\n",
    "os.environ['DASHSCOPE_API_KEY'] = apikey\n",
    "\n",
    "# Get Noto JP font to display janapese characters\n",
    "# !apt-get install fonts-noto-cjk  # For Noto Sans CJK JP\n",
    "\n",
    "#!apt-get install fonts-source-han-sans-jp # For Source Han Sans (Japanese)\n",
    "\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def parse_json(json_output):\n",
    "    # Parsing out the markdown fencing\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
    "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
    "            break  # Exit the loop once \"```json\" is found\n",
    "    return json_output\n",
    "\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "#  base 64 encode format image\n",
    "def encode_image(fra):\n",
    "    _, buffer = cv2.imencode(\".jpg\", fra)\n",
    "    return base64.b64encode(buffer).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def inference_with_api(frame, prompt, sys_prompt=\"You are a helpful assistant.\", model_id=\"qwen2.5-vl-72b-instruct\", min_pixels=512*28*28, max_pixels=2048*28*28):\n",
    "    base64_image = encode_image(frame)\n",
    "    client = OpenAI(\n",
    "        #If the environment variable is not configured, please replace the following line with the Dashscope API Key: api_key=\"sk-xxx\".\n",
    "        api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    )\n",
    "\n",
    "\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\":\"text\",\"text\": sys_prompt}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"min_pixels\": min_pixels,\n",
    "                    \"max_pixels\": max_pixels,\n",
    "                    # Pass in BASE64 image data. Note that the image format (i.e., image/{format}) must match the Content Type in the list of supported images. \"f\" is the method for string formatting.\n",
    "                    # PNG image:  f\"data:image/png;base64,{base64_image}\"\n",
    "                    # JPEG image: f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    # WEBP image: f\"data:image/webp;base64,{base64_image}\"\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model_id,\n",
    "        messages = messages,\n",
    "\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Use an API-based approach to inference. Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
    "from qwen_vl_utils import smart_resize\n",
    "\n",
    "\n",
    "min_pixels = 512*28*28\n",
    "max_pixels = 2048*28*28\n",
    "\n",
    "# Define a list of colors\n",
    "colors = [\n",
    "'red',\n",
    "'green',\n",
    "'blue',\n",
    "'yellow',\n",
    "'orange',\n",
    "'pink',\n",
    "'purple',\n",
    "'brown',\n",
    "'gray',\n",
    "'beige',\n",
    "'turquoise',\n",
    "'cyan',\n",
    "'magenta',\n",
    "'lime',\n",
    "'navy',\n",
    "'maroon',\n",
    "'teal',\n",
    "'olive',\n",
    "'coral',\n",
    "'lavender',\n",
    "'violet',\n",
    "'gold',\n",
    "'silver',\n",
    "] + additional_colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5350c9-0522-47cb-808c-1cc71c1d084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# _, i = cv2.VideoCapture(1).read()\n",
    "# cv2.imwrite(\"captured_image.jpg\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbee1b-b92f-44cc-9faa-e134bdb5aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take picture and send to Qwen model\n",
    "import cv2\n",
    "from PIL import Image\n",
    "# turn on camera\n",
    "cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    print(\"Unable to open camera\")\n",
    "    exit()\n",
    "\n",
    "# while True:\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Unable to read frame\")\n",
    "    # break\n",
    "image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "width, height = image.size\n",
    "print(width, height)\n",
    "input_height,input_width = smart_resize(height,width,min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "response = inference_with_api(frame, prompt, min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "print(response)\n",
    "\n",
    "\n",
    "im = image\n",
    "bounding_boxes = response\n",
    "# Load the image\n",
    "# img = im\n",
    "\n",
    "\n",
    "# Parsing out the markdown fencing\n",
    "bounding_boxes = parse_json(bounding_boxes)\n",
    "\n",
    "# font = ImageFont.truetype(\"NotoSansCJK-Regular.ttc\", size=14)\n",
    "\n",
    "try:\n",
    "  json_output = ast.literal_eval(bounding_boxes)\n",
    "except Exception as e:\n",
    "  end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
    "  truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
    "  json_output = ast.literal_eval(truncated_text)\n",
    "\n",
    "start, start_box = json_output[\"start\"], json_output[\"start_xyxy\"]\n",
    "end, end_box = json_output[\"end\"], json_output[\"end_xyxy\"]\n",
    "\n",
    "# Convert normalized coordinates to absolute coordinates\n",
    "# start object\n",
    "start_abs_y1 = int(start_box[0][1]/input_height * height)\n",
    "start_abs_x1 = int(start_box[0][0]/input_width * width)\n",
    "start_abs_y2 = int(start_box[1][1]/input_height * height)\n",
    "start_abs_x2 = int(start_box[1][0]/input_width * width)\n",
    "start_x_center = int((start_abs_x1 + start_abs_x2) / 2)\n",
    "start_y_center = int((start_abs_y1 + start_abs_y2) / 2)\n",
    "# start_y_center = max(start_abs_y1, start_abs_y2)\n",
    "\n",
    "if start_abs_x1 > start_abs_x2:\n",
    "    start_abs_x1, start_abs_x2 = start_abs_x2, start_abs_x1\n",
    "\n",
    "if start_abs_y1 > start_abs_y2:\n",
    "    start_abs_y1, start_abs_y2 = start_abs_y2, start_abs_y1\n",
    "\n",
    "\n",
    "# end object\n",
    "end_abs_y1 = int(end_box[0][1]/input_height * height)\n",
    "end_abs_x1 = int(end_box[0][0]/input_width * width)\n",
    "end_abs_y2 = int(end_box[1][1]/input_height * height)\n",
    "end_abs_x2 = int(end_box[1][0]/input_width * width)\n",
    "end_x_center = int((end_abs_x1 + end_abs_x2) / 2)\n",
    "end_y_center = int((end_abs_y1 + end_abs_y2) / 2)\n",
    "# end_y_center = max(end_abs_y1, end_abs_y2)\n",
    "\n",
    "if end_abs_x1 > end_abs_x2:\n",
    "    end_abs_x1, end_abs_x2 = end_abs_x2, end_abs_x1\n",
    "\n",
    "if end_abs_y1 > end_abs_y2:\n",
    "    end_abs_y1, end_abs_y2 = end_abs_y2, end_abs_y1\n",
    "\n",
    "print(f\"start position: ({start_abs_x1}, {start_abs_y1}) ({start_abs_x2}, {start_abs_y2}), center coordinate: ({start_x_center}, {start_y_center})\")\n",
    "print(f\"end position: ({end_abs_x1}, {end_abs_y1}) ({end_abs_x2}, {end_abs_y2}), center coordinate: ({end_x_center}, {end_y_center})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c76d9-2fd3-4d1f-8e56-57cfcae87ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_flag = True\n",
    "if draw_flag:\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    # Draw the bounding box\n",
    "    draw.rectangle(\n",
    "      ((start_abs_x1, start_abs_y1), (start_abs_x2, start_abs_y2)), outline=colors[0], width=4\n",
    "    )\n",
    "    # draw center of start object\n",
    "    draw.ellipse((start_x_center - 10, start_y_center - 10, start_x_center + 10, start_y_center + 10), fill=colors[0])\n",
    "\n",
    "    # draw.text((start_abs_x1 + 8, start_abs_y1 + 6), start, fill=colors[0], font=font)\n",
    "    # Draw the bounding box\n",
    "    draw.rectangle(\n",
    "      ((end_abs_x1, end_abs_y1), (end_abs_x2, end_abs_y2)), outline=colors[1], width=4\n",
    "    )\n",
    "    # draw center of end object\n",
    "    draw.ellipse((end_x_center - 10, end_y_center - 10, end_x_center + 10, end_y_center + 10), fill=colors[1])\n",
    "    # draw.text((end_abs_x1 + 8, end_abs_y1 + 6), end, fill=colors[1], font=font)\n",
    "\n",
    "    image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800fc09ccbddce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect mycobot 320\n",
    "from pymycobot import MyCobot320Socket\n",
    "import time\n",
    "def get_ip_config():\n",
    "    # 读取 YAML 文件\n",
    "    with open('env/configs.yaml', 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "\n",
    "    # 读取 IP 和端口信息\n",
    "    ip_address = data['ip']\n",
    "    netport = data['port']\n",
    "\n",
    "    return ip_address, netport\n",
    "\n",
    "\n",
    "ip_address, netport = get_ip_config()\n",
    "mc = MyCobot320Socket(ip_address, netport)\n",
    "time.sleep(1)\n",
    "\n",
    "mc.focus_all_servos()\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"\\n---> set_gripper_mode(0) => pass-through\")\n",
    "ret_mode = mc.set_gripper_mode(0)\n",
    "print(\"     Return code:\", ret_mode)\n",
    "time.sleep(1)\n",
    "\n",
    "home_angles = [0, 0, 0, 0, 0, 0]\n",
    "print(\"\\n---> Move to home position:\", home_angles)\n",
    "mc.send_angles(home_angles, 30)\n",
    "time.sleep(3)\n",
    "\n",
    "speed = 30\n",
    "\n",
    "print(\"\\n---> Open gripper\")\n",
    "mc.set_gripper_state(0, 100)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ba5022-c046-4200-a53d-2288c9db8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured camera start coordinate from vision: (474,305)\n",
      "Captured camera end coordinate from vision: (112,328)\n",
      "Converted robot start (x, y): (-249.444399453102, -133.9548873570601)\n",
      "Converted robot end (x, y): (-270.08980670667944, 51.93120432826099)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Captured camera start coordinate from vision: ({start_x_center},{start_y_center})\" )\n",
    "print(f\"Captured camera end coordinate from vision: ({end_x_center},{end_y_center})\" )\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "H = np.array([\n",
    "    [6.60782927e-04,  2.48469514e+00, -5.96091742e+02],\n",
    "    [3.82506417e-01,  4.06164160e-01, -2.18163280e+02],\n",
    "    [9.21284300e-05, -5.55189057e-03,  1.00000000e+00]\n",
    "])\n",
    "\n",
    "def convert_camera_to_robot(camera_coord, H):\n",
    "    u, v = camera_coord\n",
    "    point_h = np.array([u, v, 1.0])\n",
    "    robot_h = H.dot(point_h)\n",
    "    robot_h /= robot_h[2]\n",
    "    return (robot_h[0], robot_h[1])\n",
    "\n",
    "start_robot_xy = convert_camera_to_robot((start_x_center, start_y_center), H)\n",
    "print(\"Converted robot start (x, y):\", start_robot_xy)\n",
    "end_robot_xy = convert_camera_to_robot((end_x_center, end_y_center), H)\n",
    "print(\"Converted robot end (x, y):\", end_robot_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8443537-1aab-4eeb-b332-36e11c78d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick coordinates: [-249.444399453102, -133.9548873570601, 165, -179.46, -6.69, 95.57]\n",
      "\n",
      "---> set_gripper_mode(0) => pass-through\n",
      "     Return code: -1\n",
      "\n",
      "---> Move to home position: [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "---> Open gripper\n",
      "\n",
      "---> Move to pick coordinates\n",
      "[-249.444399453102, -133.9548873570601, 165, -179.46, -6.69, 95.57]\n",
      "\n",
      "---> Close gripper to grasp block\n",
      "\n",
      "---> Ascend after grasping (z + 150)\n",
      "[-249.444399453102, -133.9548873570601, 215, -179.46, -6.69, 95.57]\n",
      "\n",
      "---> Move to place coordinates\n",
      "[-270.08980670667944, 51.93120432826099, 215, -179.46, -6.69, 95.57]\n",
      "\n",
      "---> Open gripper to release block\n",
      "\n",
      "---> Return to home position\n",
      "\n",
      "---> Close gripper (final state)\n",
      "\n",
      "Pick & Place sequence completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pick_z = 165\n",
    "pick_orientation = [-179.46, -6.69, 95.57]\n",
    "pick_coords = [start_robot_xy[0], start_robot_xy[1], pick_z] + pick_orientation\n",
    "print(\"Pick coordinates:\", pick_coords)\n",
    "\n",
    "place_coords = [end_robot_xy[0], end_robot_xy[1], pick_z+50] + pick_orientation\n",
    "\n",
    "\n",
    "print(\"\\n---> Move to pick coordinates\")\n",
    "print(pick_coords)\n",
    "mc.send_coords(pick_coords, speed, 1)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "print(\"\\n---> Close gripper to grasp block\")\n",
    "mc.set_gripper_state(1, 100)\n",
    "time.sleep(2)\n",
    "\n",
    "pick_coords_ascend = [pick_coords[0], pick_coords[1], pick_coords[2] + 50] + pick_orientation\n",
    "print(\"\\n---> Ascend after grasping (z + 150)\")\n",
    "print(pick_coords_ascend)\n",
    "mc.send_coords(pick_coords_ascend, speed, 1)\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"\\n---> Move to place coordinates\")\n",
    "print(place_coords)\n",
    "mc.send_coords(place_coords, speed, 1)\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"\\n---> Open gripper to release block\")\n",
    "mc.set_gripper_state(0, 100)\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"\\n---> Return to home position\")\n",
    "mc.send_angles(home_angles, 30)\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"\\n---> Close gripper (final state)\")\n",
    "mc.set_gripper_state(1, 100)\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"\\nPick & Place sequence completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e3f21e-aa87-484b-9099-fded35fcc298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del mc\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e9dbc-ed34-48e2-8155-3298a50d8653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
